핵심 비교 요약

| 구분            | Baseline MLP                                | DynamicGate-MLP (DG-MLP)                                                                   |
| ------------  - | ------------------------------------------- | ------------------------------------------------------------------------------------------ |
| 층 구조          | `Linear(784→256) + ReLU + Linear(256→10)`   | 동일 + 각 Linear에 **게이트 행렬 G** (weight와 동일 shape)                                             |
| Forward         | (y=xW^\top+b)                               | (y=x(W\odot G_\text{hard})^\top+b) (학습은 STE로 (G_\text{prob})에 그래디언트 전달)                    |
| 파라미터 수      | **203,530** (= 784×256 + 256 + 256×10 + 10) | **406,794** (= Baseline + gate_logit 203,264)                                              |
| 추가 항          | 없음                                        | 게이트 파라미터 `gate_logit∈ℝ^{out×in}`, (\tau) 임계, sparsity 규제((β\cdot\text{L1}(G_\text{prob}))) |
| 연산량(MAC/샘플) | **203,264** (= 784×256 + 256×10)            | **활성 비율 r에 비례:** (r×203{,}264) (r = 활성 연결 비율)                                              |
| 정확도 경향      | 표준 MLP 수준                                | r을 너무 줄이면 정확도↓, 적절한 β/τ로 **연산량↓ vs 정확도 유지** 절충 가능                                          |
| 해석 가능성      | 가중치 크기                                  | **어떤 연결이 “켜졌는지”**(G_hard 0/1)와 “확률”(**G_prob**)로 연결 중요도 파악 용이                              |
| 저장 산출물      | fc1/fc2 **weight/bias**                     | Baseline 저장물 + **fc1/fc2 gate_prob, gate_hard** (행렬 CSV)                                   |


기준 수치
• 파라미터: 784×256=200,704 / 256×10=2,560 → Baseline 총 203,530
• MAC(곱-덧셈): 203,264/샘플 (Bias는 무시)
• DG-MLP는 게이트 파라미터(+203,264) 때문에 학습 시 파라미터는 증가하지만, 추론 시 활성 연결(r) 만큼만 곱셈을 하게 되어 실행 연산량이 감소합니다.

구조 & 수식
1) Baseline MLP
	입력: x∈R^(B×784)(28×28 평탄화)
	은닉층: h="ReLU"(xW_1^⊤+b_1), W_1∈R^(256×784),b_1∈R^256
	출력층: y=hW_2^⊤+b_2, W_2∈R^(10×256),b_2∈R^10
2) DynamicGate-MLP
	게이트 파라미터: 각 선형층 가중치와 동일한 shape의 gate_logit
G_"prob" =σ("gate_logit")∈(0,1)^("out" ×"in" )
	하드 게이트(Forward):
G_"hard" =1[G_"prob" >τ]∈{0,1}^("out" ×"in" )
	STE(Backward): G=G_"hard" +(G_"prob" -G_"prob" ."detach"())
(forward는 0/1, backward는 확률에 그래디언트가 흐르게)
	유효 가중치: W_"eff" =W⊙G
	연산: h="ReLU"(xW_(1,"eff" )^⊤+b_1),y=hW_(2,"eff" )^⊤+b_2


학습 목표(손실) 차이
항목	Baseline	DG-MLP
기본 손실	(\mathcal{L}_\text{CE}(y,,\text{label}))	(\mathcal{L}\text{CE}(y,,\text{label}) ;+; β\cdot \big(\text{mean}(G{1,\text{prob}})+\text{mean}(G_{2,\text{prob}})\big))
의미	분류 정확도만 최적화	게이트 평균을 낮추도록 유도 → 연결 희소화(연산량↓)
튜닝 포인트	LR, weight decay, epoch	β(희소화 강도), τ(임계), 초기화(gate_logit), LR 등

연산량/파라미터 상세
파라미터 수(고정 차원 예시: HIDDEN=256)
층	가중치	바이어스	합계
fc1 (784→256)	784×256 = 200,704	256	200,960
fc2 (256→10)	256×10 = 2,560	10	2,570
Baseline 합계	203,264	266	203,530

DG-MLP의 gate_logit 수 = fc1(200,704) + fc2(2,560) = 203,264
→ DG-MLP 총 파라미터 ≈ 203,530 + 203,264 = 406,794

MAC(곱-덧셈) 수(샘플당)
모델	fc1	fc2	합계
Baseline	784×256 = 200,704	256×10 = 2,560	203,264
DG-MLP	r×200,704	r×2,560	r×203,264
	예시: 활성 비율 r=0.30이라면 DG-MLP ≈ 60,979 MAC/샘플 (Baseline의 약 30%)
	실제 r값은 **학습 후 측정(게이트 하드 1의 비율)**로 결정됩니다. (제가 드린 코드에서 Gates Active Ratio로 출력)
________________________________________
저장물(Drive 경로 예시)
모델	파일(각 epoch마다)	의미
Baseline	fc1_weight.csv, fc1_bias.csv, fc2_weight.csv, fc2_bias.csv	가중치/바이어스 원본
DG-MLP	위 4개 + fc1_gate_prob.csv, fc1_gate_hard.csv, fc2_gate_prob.csv, fc2_gate_hard.csv	게이트 확률/이진(0/1) 행렬
이전 안내대로 Drive에 마운트하고 SAVE_DIR = Path("/content/drive/MyDrive/…")로 두시면 세션 종료 후에도 보존됩니다.
________________________________________
장단점 & 사용 시나리오
관점	Baseline 장점	DG-MLP 장점
학습 안정성	단순, 튜닝 쉬움	β/τ 조절로 연산량(활성 연결) 직접 제어
추론 속도	표준	게이트 희소화로 연산량↓ → 추론속도↑ (하드웨어/라이브러리의 희소 행렬 최적화 활용 시 효과 극대화)
메모리/모델 크기	작음	학습 시 gate_logit 때문에 큼 (추론 시엔 하드게이트만 남겨 프루닝/압축 가능)
해석 가능성	가중치 크기 기반	**연결 중요도(확률)와 선택(0/1)**를 직접 확인 가능
위험 요소	과적합 등 일반 이슈	β/τ 세기가 과하면 정확도 하락, STE 근사로 학습 난이도↑

무엇을 모니터링할까?
	정확도 곡선(train/test)
	Gates Active Ratio(epoch별) — 너무 급격히 떨어지면 정확도 급락 위험
	gate_prob의 분포 — 0/1 양극화가 잘 되는지 (히스토그램)
	연산량 추정 — r로부터 MAC을 환산해 속도 이득 추정
	결과 재현성 — gate 초기화/시드에 민감할 수 있으니 시드 고정
________________________________________
실험 읽는 법(제가 드린 코드 출력)
	===== 최종 비교표 =====
	Final Test Acc: 최종 테스트 정확도
	Gates Active Ratio: DG-MLP 활성 연결 비율 r
	Gates Active (/#total): 켜진 연결 수 / 총 연결 수
	위에서 r을 이용해 추론 MAC ≈ r×203,264로 환산 → Baseline 대비 %를 곁들이면 속도 절감 효익을 직감적으로 비교 가능
________________________________________
실무 팁
	초기 설정: τ=0.3,β=1e" ⁣"-" ⁣" 4∼1e" ⁣"-" ⁣" 3부터 시작 → 정확도 손실이 0.2~0.5%p 이내인 r을 탐색
	스케줄링: 초반엔 β낮게, 중/후반에 점증 (또는 τ↑) → 성능 유지하며 점진 희소화
	배치/프레임워크 최적화: 희소 행렬을 실제로 빠르게 돌리려면 희소 커널/프루닝 후 dense 재배치 등 엔진 최적화가 중요
	배포: 학습 후 G_hard로 고정 프루닝하여 작은 dense 네트워크로 내보내면(ONNX/TorchScript) 런타임 구현이 쉬워집니다.
