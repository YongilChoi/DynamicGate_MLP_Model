Colab share link : 
https://colab.research.google.com/drive/17zG4KVomGABRm2jawqIA-JmxGH-81x3Q?usp=sharing



# ============================================
# MNIST: Baseline / Dropout / Pruned / Gate-MLP
# í†µí•© ì‹¤í—˜ ì½”ë“œ (Google Colab Friendly)
# ============================================

import random, torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import pandas as pd

# -------------------- ì„¤ì • --------------------
epochs = 50
batch_size = 128
lr = 1e-3
hidden = 256
gate_beta = 1e-3
tau = 0.3
prune_ratio = 0.3   # Pruned MLPì—ì„œ ì œê±°í•  ë¹„ìœ¨
seed = 42

# -------------------- ì‹œë“œ ê³ ì • --------------------
def set_seed(seed=42):
    print("ì‹œë“œ ê°’:", seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(seed)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -------------------- ëª¨ë¸ ì •ì˜ --------------------
class MLP(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=256, output_dim=10):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

class DropoutMLP(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=256, output_dim=10, p=0.5):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.drop = nn.Dropout(p)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.drop(x)
        return self.fc2(x)

class GateLayer(nn.Module):
    def __init__(self, size, tau=0.5):
        super().__init__()
        # self.gate_logits = nn.Parameter(torch.zeros(size))
        # í•™ìŠµ ê°€ëŠ¥í•œ ê²Œì´íŠ¸ ë¡œì§“ ê°’ (ì´ˆê¸°ì—ëŠ” ëª¨ë‘ 2.0 â†’ sigmoidâ‰ˆ0.88 â†’ ê±°ì˜ ì—´ë¦¼)
        self.gate_logits = nn.Parameter(torch.ones(size) * 2.0)  # â†’ ì´ˆê¸°ì—ëŠ” ëŒ€ë¶€ë¶„ ê²Œì´íŠ¸=1 (ì—°ê²° ì—´ë¦¼).
        self.tau = tau

# 1. self.gate_logits

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° (ê¸¸ì´ = size, ì¦‰ ì€ë‹‰ ë‰´ëŸ° ê°œìˆ˜ì™€ ê°™ìŒ)

# ì´ˆê¸°ê°’ì„ 2.0ìœ¼ë¡œ ì£¼ì—ˆê¸° ë•Œë¬¸ì—,
# sigmoid(2.0) â‰ˆ 0.88 â†’ ëŒ€ë¶€ë¶„ ê²Œì´íŠ¸ëŠ” â€œì—´ë¦¼(1ì— ê°€ê¹Œì›€)â€ ìƒíƒœë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

# í•™ìŠµ ê³¼ì •ì—ì„œ optimizerê°€ ì´ ê°’ì„ ì—…ë°ì´íŠ¸ â†’ ê²Œì´íŠ¸ê°€ ë‹«íˆê±°ë‚˜ ì—´ë¦´ ìˆ˜ ìˆìŒ.
#
    def forward(self, x):
        # (1) ê²Œì´íŠ¸ í™•ë¥  ê³„ì‚° (0~1 ì‚¬ì´)
      #
      #  2. probs = torch.sigmoid(self.gate_logits)

      #   ê° ë‰´ëŸ°ë§ˆë‹¤ **ê²Œì´íŠ¸ í™•ë¥ (0~1)**ì„ ë§Œë“­ë‹ˆë‹¤.

      #   ì˜ˆ: ì–´ë–¤ ë‰´ëŸ°ì˜ gate_logit=0ì´ë©´ sigmoid(0)=0.5 â†’ ì—´ë¦´ í™•ë¥  50%

        probs = torch.sigmoid(self.gate_logits)


        # (2) hard threshold ì ìš© â†’ 0 ë˜ëŠ” 1
        # """
        # 3. hard = (probs > self.tau).float()

        # í™•ë¥ ê°’ì„ ì‹¤ì œ 0/1 ì´ì§„ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        # tauëŠ” threshold (ê¸°ë³¸ 0.5)

        # probs > 0.5 â†’ ê²Œì´íŠ¸ ì—´ë¦¼(1)

        # probs â‰¤ 0.5 â†’ ê²Œì´íŠ¸ ë‹«í˜(0)

        # ì¦‰, forward ì‹œì—ëŠ” ë‰´ëŸ°ì„ ì‚´ë¦´ì§€(1) ì£½ì¼ì§€(0) ê²°ì •í•©ë‹ˆë‹¤.
        # """

        hard = (probs > self.tau).float()

        # (3) Straight-Through Estimator ë°©ì‹: forwardì—ì„œëŠ” hardê°’ì„ ì“°ë˜, backwardì—ì„œëŠ” í™•ë¥ ì  gradientê°€ í˜ëŸ¬ê°€ê²Œ í•¨
# """
# 4. gates = hard + (probs - hard).detach()

# ì—¬ê¸°ì„œ ì¤‘ìš”í•œ íŠ¸ë¦­ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤ â†’ Straight-Through Estimator (STE)

# forward ê³„ì‚°ì—ì„œëŠ” hard (ì¦‰, 0/1 ê°’) ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ë§Œ,

# backward ì‹œ gradientëŠ” probsì—ì„œ ì˜¨ ë¯¸ë¶„ê°’ì„ í˜ë ¤ë³´ëƒ…ë‹ˆë‹¤.

# ğŸ‘‰ ì´ë ‡ê²Œ í•˜ë©´ â€œê²Œì´íŠ¸ëŠ” ì´ì§„ê°’â€ìœ¼ë¡œ ë™ì‘í•˜ë©´ì„œë„, í•™ìŠµ ì‹œì—ëŠ” ë¶€ë“œëŸ½ê²Œ gradient ì—…ë°ì´íŠ¸ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.



# """
        gates = hard + (probs - hard).detach()

        # (4) ì…ë ¥ì— ê²Œì´íŠ¸ ì ìš© (ë‰´ëŸ°ë§ˆë‹¤ ê³±í•´ì¤Œ)
# """
# 5. return x * gates, probs

# ì‹¤ì œë¡œëŠ” ì…ë ¥ xì˜ ê° ë‰´ëŸ°ì— ëŒ€í•´ ê²Œì´íŠ¸(0/1)ë¥¼ ê³±í•©ë‹ˆë‹¤.

# gate=1 â†’ ë‰´ëŸ° í†µê³¼

# gate=0 â†’ ë‰´ëŸ° ì°¨ë‹¨

# ë™ì‹œì—, í•™ìŠµ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ **í™•ë¥ (probs)**ë„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.

# """
        return x * gates, probs

# """
# GateLayerëŠ” ë‰´ëŸ° ë‹¨ìœ„ì˜ ON/OFF ìŠ¤ìœ„ì¹˜ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(gate_logits)ë¡œ ë§Œë“¤ì–´,

# forward ì‹œì—” ë‰´ëŸ°ì„ ê»ë‹¤ ì¼°ë‹¤(0/1),

# backward ì‹œì—” gradientê°€ ì˜ í˜ëŸ¬ê°€ë„ë¡ í•˜ëŠ” ë‰´ëŸ° ì„ íƒ ë ˆì´ì–´ì…ë‹ˆë‹¤.

# ë¹„ìŠ·í•œ ê°œë…

# ë“œë¡­ì•„ì›ƒ(Dropout): ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ°ì„ ëŠìŒ (í•™ìŠµ ì¤‘ë§Œ)

# ê²Œì´íŠ¸ë ˆì´ì–´: í•™ìŠµì„ í†µí•´ ì–´ë–¤ ë‰´ëŸ°ì„ ëŠì„ì§€ ê²°ì • (í›ˆë ¨ í›„ì—ë„ êµ¬ì¡°ê°€ ê³ ì • ê°€ëŠ¥ â†’ ëª¨ë¸ ì••ì¶•, pruning ìœ ì‚¬ íš¨ê³¼)
# """

# """
# #ì€ë‹‰-ì¶œë ¥ì—ë§Œ ê²Œì´íŠ¸ ì ìš©
# class DynamicGateMLP(nn.Module):
#     def __init__(self, input_dim=784, hidden_dim=256, output_dim=10, tau=0.5):
#         super().__init__()
#         self.fc1 = nn.Linear(input_dim, hidden_dim)  # ì…ë ¥ â†’ ì€ë‹‰ (Linear)
#         self.gate = GateLayer(hidden_dim, tau)
#         self.fc2 = nn.Linear(hidden_dim, output_dim)  # ì€ë‹‰ â†’ ì¶œë ¥
#     def forward(self, x):
#         x = x.view(x.size(0), -1)
#         x = self.fc1(x)
#         x, probs = self.gate(x) # ì€ë‹‰ ì¶œë ¥ì— ê²Œì´íŠ¸ ì ìš©
#         x = F.relu(x)  # í™œì„±í™”
#         return self.fc2(x), probs
# """
# ì…ë ¥-ì€ë‹‰, ì€ë‹‰-ì¶œë ¥ ëª¨ë‘ ê²Œì´íŠ¸ ì ìš©
class DynamicGateMLP(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=256, output_dim=10, tau=0.5):
        super().__init__()
        self.gate_in  = GateLayer(input_dim, tau)     # ì…ë ¥ ê²Œì´íŠ¸
        self.fc1      = nn.Linear(input_dim, hidden_dim)
        self.gate_hid = GateLayer(hidden_dim, tau)    # ì€ë‹‰ ê²Œì´íŠ¸
        self.fc2      = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x, _ = self.gate_in(x)         # ì…ë ¥ ì°¨ì› ê²Œì´íŠ¸
        x = self.fc1(x)
        x, probs = self.gate_hid(x)    # ì€ë‹‰ ì°¨ì› ê²Œì´íŠ¸
        x = F.relu(x)
        return self.fc2(x), probs

# -------------------- ë°ì´í„° --------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
train_ds = datasets.MNIST("./data", train=True, download=True, transform=transform)
test_ds  = datasets.MNIST("./data", train=False, download=True, transform=transform)
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=512, shuffle=False)

# -------------------- í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ --------------------
def train_epoch(model, loader, opt, device, crit, gate_beta=0.0):
    model.train(); total, correct = 0, 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        opt.zero_grad()
        out = model(x)
        if isinstance(out, tuple):
            logits, probs = out
            loss = crit(logits, y) + gate_beta * probs.mean()
        else:
            logits = out
            loss = crit(logits, y)
        loss.backward(); opt.step()
        correct += (logits.argmax(1) == y).sum().item()
        total += y.size(0)
    return correct / total

@torch.no_grad()
def test_epoch(model, loader, device, crit):
    model.eval(); total, correct = 0, 0; gate_probs = []
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        out = model(x)
        if isinstance(out, tuple):
            logits, probs = out
            gate_probs.append(probs.mean().item())
        else:
            logits = out
        correct += (logits.argmax(1) == y).sum().item()
        total += y.size(0)
    return correct / total, (sum(gate_probs)/len(gate_probs) if gate_probs else None)

# -------------------- FLOPs & Params ê³„ì‚° --------------------
def compute_stats(model, active_ratio=None):
    params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    # FLOPs ê·¼ì‚¬ (fc1 + fc2)
    if hasattr(model, 'fc1'):
        flops = 784*hidden + hidden*10
    else:
        flops = 0
    if active_ratio is not None:
        flops = int(flops * active_ratio)
    return params, flops

# -------------------- Pruning í•¨ìˆ˜ --------------------
def apply_pruning(model, ratio=0.3):
    with torch.no_grad():
        weight = model.fc1.weight.abs()
        thresh = torch.quantile(weight.view(-1), ratio)
        mask = (weight > thresh).float()
        model.fc1.weight *= mask
    return model

# -----------ì¶”ê°€í•  í•¨ìˆ˜: show_gate_matrix ----------------
def show_gate_matrix(model, rows=8, cols=8):
    # """
    # DynamicGate-MLP ëª¨ë¸ì˜ GateLayer ê°’ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    # - í™•ë¥ ê°’ (0~1)
    # - Hard ê°’ (0/1)
    # - ì „ì²´ í•©ê³„
    # - í–‰ë ¬ ë° íˆíŠ¸ë§µ
    # """
    if not hasattr(model, "gate"):  # ì€ë‹‰ì¸µ ë‹¤ìŒì— ê²Œì´íŠ¸ë ˆì´ì–´ë§Œ ì ìš©ì‹œì—
        print("This model has no GateLayer.")
        return

    # if not hasattr(model, "gate_in"):
    #     print("This model has no GateLayer.")
    #     return
    with torch.no_grad():
        probs = torch.sigmoid(model.gate.gate_logits).cpu().numpy()
        hard = (probs > model.gate.tau).astype(int)

    # DataFrame ì¶œë ¥
    df_probs = pd.DataFrame(probs.reshape(1, -1))
    df_hard  = pd.DataFrame(hard.reshape(1, -1))

    print("\nGate probabilities (0~1):")
    display(df_probs)

    print("\nGate hard values (0/1):")
    display(df_hard)

    print(f"\nActive gates: {df_hard.values.sum()} / {df_hard.size}")

    # í–‰ë ¬ í˜•íƒœë¡œ ë³€í™˜
    try:
        hard_matrix = hard.reshape(rows, cols)
        plt.imshow(hard_matrix, cmap="Greens", aspect="auto")
        plt.colorbar(label="Gate (0=off, 1=on)")
        plt.title("DynamicGate-MLP Gate Status (0=off, 1=on)")
        plt.show()
    except Exception as e:
        print("Cannot reshape to matrix:", e)

# -------------------- ì‹¤í–‰ --------------------
crit = nn.CrossEntropyLoss()
results = []

# Baseline MLP
mlp = MLP(hidden_dim=hidden).to(device)
opt = torch.optim.Adam(mlp.parameters(), lr=lr)
for e in range(epochs):
    tr = train_epoch(mlp, train_loader, opt, device, crit)
    te, _ = test_epoch(mlp, test_loader, device, crit)
print("Baseline MLP acc:", te)
params, flops = compute_stats(mlp)
results.append(["Baseline MLP", te*100, 0, 0, params, flops])

# Dropout MLP
drop = DropoutMLP(hidden_dim=hidden, p=0.5).to(device)
opt_d = torch.optim.Adam(drop.parameters(), lr=lr)
for e in range(epochs):
    tr = train_epoch(drop, train_loader, opt_d, device, crit)
    te, _ = test_epoch(drop, test_loader, device, crit)
print("Dropout MLP acc:", te)
params, flops = compute_stats(drop)
results.append(["Dropout MLP", te*100, 0, 0, params, flops])

# Pruned MLP
pruned = MLP(hidden_dim=hidden).to(device)
pruned = apply_pruning(pruned, prune_ratio)
opt_p = torch.optim.Adam(pruned.parameters(), lr=lr)
for e in range(epochs):
    tr = train_epoch(pruned, train_loader, opt_p, device, crit)
    te, _ = test_epoch(pruned, test_loader, device, crit)
print("Pruned MLP acc:", te)
params, flops = compute_stats(pruned, active_ratio=(1-prune_ratio))
results.append(["Pruned MLP", te*100, prune_ratio*100, prune_ratio*100, params, flops])

# DynamicGate-MLP
gate = DynamicGateMLP(hidden_dim=hidden, tau=tau).to(device)
opt_g = torch.optim.Adam(gate.parameters(), lr=lr)
for e in range(epochs):
    tr = train_epoch(gate, train_loader, opt_g, device, crit, gate_beta=gate_beta)
    te, gp = test_epoch(gate, test_loader, device, crit)
print("Gate MLP acc:", te, "mean gate:", gp)
params, flops = compute_stats(gate, active_ratio=gp)
results.append(["DynamicGate-MLP", te*100, (1-gp)*100, (1-gp)*100, params, flops])

# -------------------- ê²°ê³¼ í‘œ --------------------
df = pd.DataFrame(results, columns=["Model","Accuracy(%)","Param Reduction(%)","FLOPs Reduction(%)","Params","FLOPs"])
display(df)

# GateLayer ê°’ ë³´ê¸°
#show_gate_matrix(gate, rows=8, cols=8)   # hidden=64ì¼ ë•Œ 8x8
show_gate_matrix(gate, rows=16, cols=16) # hidden=256ì¼ ë•Œ 16x16

# -------------------- ê·¸ë˜í”„ ì‹œê°í™” --------------------
import numpy as np

# ëª¨ë¸ ì´ë¦„
models = df["Model"].tolist()

# ì •í™•ë„ ê·¸ë˜í”„
plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.bar(models, df["Accuracy(%)"], color="skyblue")
plt.title("Accuracy (%)")
plt.ylabel("Accuracy (%)")
plt.xticks(rotation=20)

# íŒŒë¼ë¯¸í„° ìˆ˜ ê·¸ë˜í”„ (ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œë„ ìœ ìš©)
plt.subplot(1,3,2)
plt.bar(models, df["Params"], color="lightgreen")
plt.title("Number of Parameters")
plt.ylabel("Params")
plt.xticks(rotation=20)

# FLOPs ê·¸ë˜í”„
plt.subplot(1,3,3)
plt.bar(models, df["FLOPs"], color="salmon")
plt.title("FLOPs (Computation Cost)")
plt.ylabel("FLOPs")
plt.xticks(rotation=20)

plt.suptitle("Model Comparison: Accuracy / Params / FLOPs", fontsize=14)
plt.tight_layout()
plt.show()




