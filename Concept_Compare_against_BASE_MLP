📘 README.md (예시)
# Baseline MLP vs DynamicGate-MLP

## 1. 개념 비교

| 항목           | Baseline MLP          | DynamicGate-MLP                      |
|----------------|-----------------------|--------------------------------------|
| 학습 대상       | Weight (W), Bias (b) | Weight (W), Bias (b) **+ Gate Logit** |
| 업데이트되는 값 | W, b                  | W, b + Gate Logit (→ Gate Prob)      |
| 게이트 파라미터 | 없음                  | Gate Prob = sigmoid(gate_logit)      |
| Forward        | y = x Wᵀ + b          | y = x (W ⊙ G_hard)ᵀ + b             |
| Backward       | ∇W, ∇b              | ∇W, ∇b + ∇gate_logit (STE)         |
| 구조 변화       | Dense (고정)          | 학습 과정에서 연결 ON/OFF (희소화)     |
| 해석 가능성     | Weight 크기만으로 추정 | Gate Prob/Hard 행렬(0/1)로 직접 확인  |

---

## 2. 공통점
- 둘 다 Weight, Bias를 학습해 분류기 역할을 한다.  
- MNIST 같은 데이터셋에서 CrossEntropy Loss로 학습 가능하다.  
- PyTorch 기반 MLP 구조로 학습/추론이 가능하다.  

---

## 3. 차이점
- **Baseline MLP**: 모든 연결이 끝까지 유지, Weight와 Bias만 학습.  
- **DynamicGate-MLP**:  
  - Gate Logit → Sigmoid → Gate Prob (0~1) → Threshold τ → Gate Hard (0/1)  
  - 학습 중 일부 연결이 꺼지면서 네트워크가 희소화됨.  
  - Forward: 꺼진 연결은 연산 자체가 제거.  
  - Backward: STE 덕분에 Gate Logit까지 gradient 전달 → 학습 가능.  

---

## 4. DynamicGate-MLP가 필요한 이유
- **추론 효율화**: 학습 후 Gate Hard(0/1)로 pruning → 모델 크기와 연산량 감소.  
- **제어 가능성**: 하이퍼파라미터 β, τ를 조절해 희소화 강도를 제어할 수 있다.  
- **해석 가능성**: 어떤 연결이 중요했는지 Gate 행렬(Prob/Hard)로 직접 확인 가능.  
- **Dropout + Pruning 융합**: Dropout처럼 동적이면서, Pruning처럼 영구적인 연결 제거 가능.  

---

## 5. 구조 비교 다이어그램

```mermaid
graph TD
    subgraph Baseline MLP
    A[입력 x (784)] --> B[fc1: 784→256 W1,b1]
    B --> C[ReLU]
    C --> D[fc2: 256→10 W2,b2]
    D --> E[출력 y]
    end

    subgraph DynamicGate-MLP
    A2[입력 x (784)] --> B2[fc1: 784→256 W1,b1]
    B2 --> G[게이트 행렬 G1 (학습)]
    G --> C2[ReLU]
    C2 --> D2[fc2: 256→10 W2,b2]
    D2 --> H[게이트 행렬 G2 (학습)]
    H --> E2[출력 y]
    end
•	왼쪽: Baseline MLP — Weight와 Bias만 학습
•	오른쪽: DynamicGate-MLP — Weight, Bias 학습 + Gate 행렬 학습
________________________________________
6. 한 줄 요약
Baseline MLP는 W, b만 학습하지만,
DynamicGate-MLP는 W, b + Gate까지 학습하여
“필요한 연결만 살아남는” 희소 구조를 만들고,
추론 효율성과 해석 가능성을 동시에 확보한다.
